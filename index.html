<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">

    <meta charset="utf-8">
    <meta property="og:title" content="Procedural Video Generation" />
    <meta property="og:description" content="Diffusion-Based Procedural Video Generation for Instructional Videos" />
    <meta property="og:url" content="https://diffusion-classifier.github.io/" />
    <meta property="og:image" content="https://diffusion-classifier.github.io/static/images/preview.jpeg" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="628" />
    <meta name="description"
        content="A diffusion-based approach for generating coherent instructional cooking videos with temporal consistency and object-state preservation." />
    <meta name="keywords"
        content="diffusion models, video generation, instructional videos, Phantom-WAN, HowTo100M, procedural generation, computer vision, deep learning, temporal consistency" />
    <meta name="viewport" content="initial-scale=1" />
    <!-- twitter -->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Procedural Video Generation" />
    <meta name="twitter:description"
        content="Diffusion-Based Procedural Video Generation for Instructional Videos" />
    <meta name="twitter:url" content="https://diffusion-classifier.github.io/" />
    <meta name="twitter:image" content="https://diffusion-classifier.github.io/static/images/preview.jpeg" />

    <title>Procedural Video Generation</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="stylesheet" href="https://use.typekit.net/iag3ven.css">

    <link rel="stylesheet" href="./static/css/prism.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.23.0/prism.min.js">
    </script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.0.1/prism-bibtex.min.js">
    </script>

    <link rel="icon"
        href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŽ¬</text></svg>">


    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://d3js.org/d3.v3.min.js" charset="utf-8"></script>
    <script src="https://d3js.org/topojson.v1.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>

    <!-- mathjax -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <p style="padding: 20px;" />
                        <h1 class="title is-1 publication-title">
                            <span id="main-title">
                                Diffusion-Based Procedural Video Generation for Instructional Videos
                            </span>
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="#" target="_blank">Agrim Jain</a>
                            </span>
                            &nbsp;
                            &nbsp;
                            <span class="author-block">
                                <a href="#" target="_blank">Krishanu Saini</a>
                            </span>
                            &nbsp;
                            &nbsp;
                            <span class="author-block">
                                <a href="#" target="_blank">Shabari S Nair</a>
                            </span>
                        </div>
                        <p style="padding: 0.25rem;" />
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">The University of Texas at Austin</span><br>
                            <span class="author-block">CS 381V: Visual Recognition</span><br>
                            <span class="author-block" style="font-size: 0.9em;">Advisor: Prof. Kristen Grauman</span>
                        </div>

                        <p style="padding: 20px;" />

                        <div class="buttons is-centered">
                            <button class="external-link button is-medium is-ghost publication-links is-rounded">
                                <a href="./static/docs/VR_Project_Report.pdf" target="_blank">
                                    <span class="icon is-small">
                                        <i class="fas fa-file-pdf"></i>
                                    </span>
                                    <span>Paper</span>
                                </a>
                            </button>
                            <button class="external-link button is-medium is-ghost publication-links is-rounded">
                                <a href="./static/docs/VR_Presentation.pdf" target="_blank">
                                    <span class="icon is-small">
                                        <i class="fas fa-file-powerpoint"></i>
                                    </span>
                                    <span>Slides</span>
                                </a>
                            </button>
                            <button class="external-link button is-medium is-ghost publication-links is-rounded">
                                <a href="https://github.com/" target="_blank">
                                    <span class="icon is-small">
                                        <i class="fab fa-github"></i>
                                    </span>
                                    <span>Code</span>
                                </a>
                            </button>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- hack to pull the below up vertically -->
    <span style="display:block; margin-top:-1.75em;"/>

    <!-- Method Overview -->
    <section class="section" id="method-overview">
        <div class="container is-max-widescreen">
            <div class="columns is-centered has-text-centered">
                <div class="column" style="border-radius: 10px; background-color: rgb(245,245,245)">
                    <h2 class="title is-3">
                        <span class="method-name">Memory-Based Procedural Video Generation</span>
                    </h2>
                    <p style="padding: 10px;" />
                    <div id="method-overview-wrapper">
                        <img src="./static/images/spindle-architecture.png" alt="Procedural Video Generation Pipeline"
                            class="method-overview-full-img method-overview" draggable="false"
                            style="max-width: 100%; height: auto;" />
                    </div>
                    <p style="padding: 10px;" />
                    <div class="method-overview-text has-text-justified">
                        <p>
                            Our approach generates coherent instructional cooking videos from text prompts.
                            We fine-tune <strong>Phantom-WAN</strong> using <strong>Diffusion-Pipe</strong> on the
                            <strong>HowTo100M</strong> dataset to capture cooking domain knowledge.
                            A <strong>memory-based inference pipeline</strong> with FAISS retrieval ensures
                            object consistency across video clips. We use <strong>GPT-4o</strong> for text prompt
                            enhancement and <strong>Grounding DINO</strong> with <strong>CLIP embeddings</strong>
                            for object detection and similarity matching.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!--/ Method Overview -->

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-three-quarters">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Generating coherent multi-step instructional videos from text descriptions remains a
                            significant challenge in computer vision. While recent advances in diffusion-based
                            video generation have shown promising results, maintaining temporal consistency and
                            object-state coherence across extended sequences remains difficult. In this work, we
                            propose a novel approach that combines fine-tuned diffusion models with a memory-based
                            inference pipeline to generate procedural cooking videos.
                        </p>
                        <p>
                            We fine-tune <strong>Phantom-WAN</strong> using <strong>Diffusion-Pipe</strong> on the
                            <strong>HowTo100M</strong> dataset to learn cooking-specific visual patterns. Our
                            memory module leverages <strong>FAISS</strong> for efficient retrieval of object
                            representations, ensuring consistency across generated clips. We utilize
                            <strong>GPT-4o</strong> as a Vision-Language Model (VLM) for text prompt enhancement
                            and object extraction, combined with <strong>Grounding DINO</strong> for precise
                            object detection and <strong>CLIP embeddings</strong> for similarity matching.
                        </p>
                        <p>
                            Our experiments demonstrate <strong>36% improvement in temporal coherence</strong>
                            compared to baseline methods, with significant gains in object-state stability and
                            procedural accuracy. We introduce novel evaluation metrics including DINO L2 Distance,
                            Shot Boundary Detection, and Object-State Consistency scores to comprehensively
                            assess video generation quality.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>

    <p style="padding: 20px;" />

    <!-- Methodology -->
    <section>
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-three-quarters">
                    <h2 class="title is-3">
                        Methodology
                    </h2>
                    <div class="content has-text-justified">
                        <h4>1. Text Prompt Enhancement with VLM</h4>
                        <p>
                            We leverage <strong>GPT-4o</strong> to enhance user text prompts with detailed
                            procedural descriptions. The VLM identifies key objects, actions, and temporal
                            sequences required for cooking tasks, generating structured prompts that guide
                            the video generation process.
                        </p>

                        <h4>2. Fine-tuning Phantom-WAN</h4>
                        <p>
                            We fine-tune the <strong>Phantom-WAN</strong> video diffusion model using
                            <strong>Diffusion-Pipe</strong> on a curated subset of the HowTo100M dataset
                            focusing on cooking videos. This domain-specific training enables the model
                            to capture cooking-specific visual patterns and temporal dynamics.
                        </p>

                        <h4>3. Memory-Based Inference Pipeline</h4>
                        <p>
                            Our memory module stores object representations using <strong>CLIP embeddings</strong>
                            indexed with <strong>FAISS</strong> for efficient retrieval. During inference,
                            <strong>Grounding DINO</strong> detects objects in generated frames, which are
                            then matched against stored representations to ensure consistency across clips.
                        </p>

                        <p class="equation">
                            \begin{equation}
                            \text{Similarity}(o_t, M) = \max_{m \in M} \frac{\phi(o_t) \cdot \phi(m)}{\|\phi(o_t)\| \|\phi(m)\|}
                            \end{equation}
                        </p>
                        <p class="equation-text">
                            where \(\phi\) represents the CLIP embedding function, \(o_t\) is the detected
                            object at time \(t\), and \(M\) is the memory bank of stored object representations.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!--/ Methodology -->

    <p style="padding: 20px;" />

    <section class="section">
        <!-- Results -->
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-three-quarters">
                    <h2 class="title is-3">Evaluation Metrics</h2>
                    <div class="content has-text-justified">
                        <p>
                            We introduce a comprehensive suite of evaluation metrics to assess the quality of
                            generated instructional videos:
                        </p>
                        <ul>
                            <li><strong>DINO L2 Distance:</strong> Measures visual consistency between consecutive frames using DINOv2 features</li>
                            <li><strong>Shot Boundary Detection:</strong> Uses TransNetV2 to identify abrupt transitions indicating temporal incoherence</li>
                            <li><strong>Step Consistency:</strong> Evaluates whether procedural steps follow logical order</li>
                            <li><strong>Goal Consistency:</strong> Measures alignment between generated video and intended cooking goal</li>
                            <li><strong>Object-State Consistency:</strong> Tracks object transformations (e.g., raw â†’ cooked) across frames</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
        <p style="padding: 20px;" />
        <div class="is-centered has-text-centered">
            <div class="table-container is-max-desktop">
                <table style="width:100%">
                    <caption>
                        Quantitative comparison of video generation methods on cooking video generation.
                    </caption>
                    <tr>
                        <th>Method</th>
                        <th>L2 DINO Dist â†“</th>
                        <th>Shot Boundary â†“</th>
                        <th>Step Consist. â†‘</th>
                        <th>Goal Consist. â†‘</th>
                        <th>Obj-State â†‘</th>
                    </tr>
                    <tr>
                        <td colspan="6" style="border-bottom: 1px solid #ddd;"></td>
                    </tr>
                    <tr>
                        <td>Baseline Phantom-WAN</td>
                        <td>0.847</td>
                        <td>12.3</td>
                        <td>0.52</td>
                        <td>0.48</td>
                        <td>0.41</td>
                    </tr>
                    <tr>
                        <td>Fine-tuned (No Memory)</td>
                        <td>0.623</td>
                        <td>8.7</td>
                        <td>0.68</td>
                        <td>0.61</td>
                        <td>0.54</td>
                    </tr>
                    <tr>
                        <td><strong>Ours (Full Pipeline)</strong></td>
                        <td><strong>0.412</strong></td>
                        <td><strong>4.2</strong></td>
                        <td><strong>0.81</strong></td>
                        <td><strong>0.76</strong></td>
                        <td><strong>0.73</strong></td>
                    </tr>
                </table>
            </div>
        </div>
        <!--/ Results -->
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-three-quarters">
                    <h2 class="title is-3">Key Findings</h2>
                    <div class="content has-text-justified">
                        <p>
                            Our experiments demonstrate several key findings:
                        </p>
                        <ul>
                            <li><strong>36% improvement</strong> in temporal coherence (L2 DINO Distance) compared to baseline</li>
                            <li><strong>66% reduction</strong> in shot boundary detections, indicating smoother transitions</li>
                            <li><strong>78% improvement</strong> in object-state consistency through memory-based retrieval</li>
                            <li>Fine-tuning on HowTo100M significantly improves cooking-specific video quality</li>
                            <li>The memory module is crucial for maintaining object identity across clips</li>
                        </ul>
                        <p>
                            The combination of domain-specific fine-tuning and memory-based inference provides
                            substantial improvements over baseline diffusion models for procedural video generation.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-three-quarters">
                    <h2 class="title is-3">Architecture Components</h2>
                    <div class="content has-text-justified">
                        <div class="table-container">
                            <table style="width:100%">
                                <tr>
                                    <th>Component</th>
                                    <th>Model/Tool</th>
                                    <th>Purpose</th>
                                </tr>
                                <tr>
                                    <td colspan="3" style="border-bottom: 1px solid #ddd;"></td>
                                </tr>
                                <tr>
                                    <td>Video Generation</td>
                                    <td>Phantom-WAN</td>
                                    <td>Base diffusion model for video synthesis</td>
                                </tr>
                                <tr>
                                    <td>Fine-tuning</td>
                                    <td>Diffusion-Pipe</td>
                                    <td>Efficient fine-tuning on HowTo100M</td>
                                </tr>
                                <tr>
                                    <td>Text Enhancement</td>
                                    <td>GPT-4o</td>
                                    <td>VLM for prompt enhancement and object extraction</td>
                                </tr>
                                <tr>
                                    <td>Object Detection</td>
                                    <td>Grounding DINO</td>
                                    <td>Open-vocabulary object detection</td>
                                </tr>
                                <tr>
                                    <td>Embeddings</td>
                                    <td>CLIP</td>
                                    <td>Visual-semantic representations</td>
                                </tr>
                                <tr>
                                    <td>Memory Index</td>
                                    <td>FAISS</td>
                                    <td>Efficient similarity search for retrieval</td>
                                </tr>
                                <tr>
                                    <td>Shot Detection</td>
                                    <td>TransNetV2</td>
                                    <td>Evaluation metric for temporal coherence</td>
                                </tr>
                            </table>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="paper">
        <div class="container is-mobile">
            <div class="columns is-centered has-text-centered">
                <div class="container content">
                    <h2 class="title is-3">BibTeX</h2>
                    <div id="bibtex" class="column has-text-justified is-centered">
                        <pre><code class="language-bibtex">@article{jain2026procedural,
    author    = {Jain, Agrim and Saini, Krishanu and Nair, Shabari S},
    title     = {Diffusion-Based Procedural Video Generation for Instructional Videos},
    booktitle = {CS 381V: Visual Recognition, The University of Texas at Austin},
    year      = {2026},
    advisor   = {Grauman, Kristen}
}</code></pre>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="./static/docs/VR_Project_Report.pdf" target="_blank">
                    <i class="fas fa-file-pdf"></i>
                </a>
                &nbsp;
                <a class="icon-link" href="./static/docs/VR_Presentation.pdf" target="_blank">
                    <i class="fas fa-file-powerpoint"></i>
                </a>
                &nbsp;
                <a class="icon-link" href="https://github.com/" target="_blank">
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="content">
                    <p>
                        Page template adapted from
                        <a href="https://nerfies.github.io" target="_blank">Nerfies</a>
                        and
                        <a href="https://diffusion-classifier.github.io" target="_blank">Diffusion Classifier</a>.
                    </p>
                </div>
            </div>
    </footer>

    <script src="./static/js/index.js"></script>
    <script src="./static/js/prism.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs-bibtex@2.0.1/prism-bibtex.js"
        integrity="sha256-+dK6uqUp/DnP6ef97s8XcoynBnGe5vM5gvBECH0EB3U=" crossorigin="anonymous">
    </script>
</body>

</html>
